---
title: "Análisis de la varianza"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(kableExtra)
load("~/material_docente/testhipotesis/zelazo.RData")
```

El término *análisis de la varianza" corresponde a un test de comparación de ... ¡medias! Aunque te parezca extraño, la hipótesis nula que se plantea es:
$$H_0: \mu_1 = \mu_2 = ... = \mu_i$$
Cuando tenemos dos grupos, la comparación de las medias la podemos hacer, aplicando el tes *t* de Student. ¿Y si tenemos más de dos grupos, por ejemplo, tres? En este caso concreto, con los grupos A, B y C, podemos comparar la media de A con la de B; la de A con la de C; y la de B con la de C.  
Esa forma de proceder, aunque intuitiva, puede sin embargo, conducirnos a conclusiones erróneas, porque se produce el llamado problema de las **comparaciones múltiples**. Al hacer varias comparaciones, en cada una de ellas podemos cometer un error $\alpha$, y una probabilidad $1-\alpha$ de no rechazar la hipótesis nula. Como los errores se multiplican, al hacer varias comparaciones (k), $1-(1-\alpha)^k$. En definitiva, al hacer varias comparaciones, es mucho más probable que digamos que hay diferencias, aunque esto no sea totalmente correcta.  
El análisis de la varianza compara simultáneamente varios grupos, sin que por ello se aumente el error $\alpha$ cometido.

## La hipótesis nula

```{r}
t2 <- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
      stat_function(fun = dnorm, args = list(x=0,sd = 1)) +
      xlab(NULL) +
      theme_bw()
t2
```

## La hipótesis alternativa
$$H_1: \mu_i \neq \mu_j, i\neq j$$
---

```{r}
t2 <- ggplot(data.frame(x = c(-4, 20)), aes(x = x)) +
      stat_function(fun = dnorm, args = list(x=0,sd = 1)) +
      stat_function(fun = dnorm, args = list(x=8,sd = 1)) +
      stat_function(fun = dnorm, args = list(x=16,sd = 1)) +
      xlab(NULL) +
      theme_bw()
t2
```
## El estadístico del contraste

$$SC_{Total} = SC_{Intergrupos}+SC_{Entregrupos} $$
$$\sum(\bar Y - y_{ij})^2 = \sum(\bar Y_i - y_{ij})^2 + \sum(\bar Y - \bar Y_i)^2$$
Y el estadístico se define del siguiente modo:
$$F_{exp} = \frac {SC_{Intergrupos}}{SC_{Entregrupos}}$$
$$F_{exp} \sim F_{{(n_1-1)},{(n_2-1)}}$$

---

```{r}
funcShaded <- function(x) {
    y <- df(x, df1=1,df2=1)
    y[x < qf(0.95,df1=1,df2=1)] <- NA
    return(y)
}
ggplot(data.frame(x = c(0, 200)), aes(x = x)) +
      stat_function(fun = df, args = list(df1 = 1,df=1)) +
      stat_function(fun=funcShaded, geom="area", fill="red") +
      scale_y_continuous("Densidad") +
      xlab(NULL) +
      theme_bw() +
      theme(axis.text.x=element_text(colour="black", size = 14))  
```

## Condición  
Normalidad de las distribuciones  

Pero ¿cómo lo compruebo?  

## Test de normalidad
H_0: la distribución es normal.  
H_1: la distribució no es normal.

## Test de Shapiro-Wilk  

```{r}
shapiro.test(zelazo$edad)
```

## Análisis de la varianza (ANOVA)  

```{r}
AnovaModel.1 <- aov(edad ~ tto, data=zelazo)
summary(AnovaModel.1)
```

---

```{r}
AnovaModelo.2 <- kruskal.test(edad ~ tto, data=zelazo)
AnovaModelo.2
```


